{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets.utils import PreprocessingDataset\n",
    "from models.utils import get_model_by_name\n",
    "from utils.environment import modified_environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "DATASET = \"Wikimedia\"\n",
    "assert DATASET in [\"UGallery\", \"Wikimedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE, NUM_WORKERS = 8, 4\n",
    "IMAGES_EXT = [\"*.gif\", \"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]\n",
    "USE_GPU = True\n",
    "\n",
    "# Model\n",
    "MODEL = \"resnet50\"\n",
    "LAYER = \"\" # if not defined the last layer, before the classification, output will be extracted\n",
    "assert MODEL in [\"alexnet\", \"vgg16\", \"resnet50\"]\n",
    "\n",
    "# Images path\n",
    "IMAGES_DIR = None\n",
    "if DATASET == \"Wikimedia\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"data2\", \"wikimedia\", \"mini-images-224-224-v2\")\n",
    "elif DATASET == \"UGallery\":\n",
    "    IMAGES_DIR = os.path.join(\"/\", \"mnt\", \"workspace\", \"Ugallery\", \"mini-images-224-224-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (output)\n",
    "LAYERED_OUTPUT = f\"-{LAYER}\" if LAYER else \"\"\n",
    "OUTPUT_EMBEDDING_PATH = os.path.join(\"data\", DATASET, f\"embedding-{MODEL}{LAYERED_OUTPUT}.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import ImageFile\n",
    "\n",
    "\n",
    "# Needed for some images in the Wikimedia dataset\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 3_000_000_000\n",
    "# Some images are \"broken\" in Wikimedia dataset\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Setting up torch device (useful if GPU available)\n",
    "print(\"\\nCreating device...\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
    "if torch.cuda.is_available() != USE_GPU:\n",
    "    print((f\"\\nNotice: Not using GPU - \"\n",
    "           f\"Cuda available ({torch.cuda.is_available()}) \"\n",
    "           f\"does not match USE_GPU ({USE_GPU})\"\n",
    "    ))\n",
    "\n",
    "# Downloading models for feature extraction\n",
    "print(\"\\nDownloading model...\")\n",
    "with modified_environ(TORCH_HOME=\".\"):\n",
    "    print(f\"Model: {MODEL} (pretrained on imagenet)\")\n",
    "    model = get_model_by_name(MODEL, output_layer=LAYER).eval().to(device)\n",
    "\n",
    "# Setting up transforms and dataset\n",
    "print(\"\\nSetting up transforms and dataset...\")\n",
    "images_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "image_dataset = PreprocessingDataset(\n",
    "    IMAGES_DIR,\n",
    "    extensions=IMAGES_EXT,\n",
    "    transform=images_transforms,\n",
    ")\n",
    "image_dataloader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "print(f\">> Images dataset: {len(image_dataset)}\")\n",
    "\n",
    "# Calculate embedding dimension size\n",
    "dummy_input = torch.ones(1, *image_dataset[0][\"image\"].size()).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "emb_dim = dummy_output.shape[1:] if LAYER else dummy_output.size(1)\n",
    "print(f\">> Embedding dimension size: {emb_dim}\")\n",
    "\n",
    "# Feature extraction phase\n",
    "print(f\"\\nFeature extraction...\")\n",
    "output_ids = np.empty(len(image_dataset), dtype=object)\n",
    "if LAYER:\n",
    "    output_embedding = torch.zeros((len(image_dataset), *emb_dim), dtype=torch.float32, device=device)\n",
    "else:\n",
    "    output_embedding = torch.zeros((len(image_dataset), emb_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_i, sample in enumerate(tqdm(image_dataloader, desc=\"Feature extraction\")):\n",
    "        item_image = sample[\"image\"].to(device)\n",
    "        item_idx = sample[\"idx\"]\n",
    "        output_ids[[*item_idx]] = sample[\"id\"]\n",
    "        output_embedding[item_idx] = model(item_image).squeeze(-1).squeeze(-1)\n",
    "\n",
    "output_embedding = output_embedding.cpu().numpy()\n",
    "\n",
    "# Fill output embedding\n",
    "embedding = np.ndarray(\n",
    "    shape=(len(image_dataset), 2),\n",
    "    dtype=object,\n",
    ")\n",
    "for i in range(len(image_dataset)):\n",
    "    embedding[i] = np.asarray([output_ids[i], output_embedding[i]])\n",
    "print(f\">> Embedding shape: {embedding.shape}\")\n",
    "\n",
    "# Save embedding to file\n",
    "print(f\"\\nSaving embedding to file... ({OUTPUT_EMBEDDING_PATH})\")\n",
    "np.save(OUTPUT_EMBEDDING_PATH, embedding, allow_pickle=True)\n",
    "\n",
    "# Free some memory\n",
    "if USE_GPU:\n",
    "    print(f\"\\nCleaning GPU cache...\")\n",
    "    model = model.to(torch.device(\"cpu\"))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Finished\n",
    "print(\"\\nDone\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

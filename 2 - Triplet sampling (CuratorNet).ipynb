{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from math import ceil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.data import (\n",
    "    extract_embedding, get_interactions_dataframe,\n",
    "    mark_evaluation_rows, get_holdout,\n",
    ")\n",
    "from utils.hashing import pre_hash, HashesContainer\n",
    "from utils.sampling import StrategyHandler\n",
    "from utils.similarity import HybridScorer, VisualSimilarityHandler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "# * UGallery: No max profile size, allow group interactions.\n",
    "# * Wikimedia: Max profile size (10), allow group interactions.\n",
    "# * Pinterest: Max profile size (10), disable group interactions. Use cache and mode big.\n",
    "# * Tradesy: Max profile size (10), disable group interactions. Use cache and mode big.\n",
    "DATASET = \"UGallery\"\n",
    "assert DATASET in [\"UGallery\", \"Wikimedia\", \"Pinterest\", \"Tradesy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode\n",
    "# Use 'MODE_PROFILE = True' for CuratorNet-like training \n",
    "# Use 'MODE_PROFILE = False' for VBPR-like training\n",
    "MODE_PROFILE = True\n",
    "MODE_PROFILE _VERBOSE = \"profile\" if MODE_PROFILE else \"user\"\n",
    "# Use 'MODE_BIG = True' for big embeddings\n",
    "# Use 'MODE_BIG = False' for regular-size embeddings\n",
    "MODE_BIG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor\n",
    "FEATURE_EXTRACTOR = \"resnet50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (general)\n",
    "EMBEDDING_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}_embedding-{FEATURE_EXTRACTOR}.npy\")\n",
    "INTERACTIONS_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}.csv\")\n",
    "OUTPUT_TRAIN_PATH = os.path.join(\"data\", DATASET, f\"{MODE_PROFILE_VERBOSE}-train.csv\")\n",
    "OUTPUT_VALID_PATH = os.path.join(\"data\", DATASET, f\"{MODE_PROFILE_VERBOSE}-validation.csv\")\n",
    "OUTPUT_EVAL_PATH = os.path.join(\"data\", DATASET, f\"{MODE_PROFILE_VERBOSE}-evaluation.csv\")\n",
    "\n",
    "# Caching\n",
    "USE_CACHE = True\n",
    "CACHE_PCA_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}_pca.npy\")\n",
    "CACHE_LABELS_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}_labels.npy\")\n",
    "\n",
    "# General constants\n",
    "RNG_SEED = 0\n",
    "\n",
    "# Visual cluster constants\n",
    "CLUSTERING_RNG = None\n",
    "CLUSTERING_N_CLUSTERS = 100\n",
    "CLUSTERING_N_INIT = 8\n",
    "CLUSTERING_N_TIMES = 20\n",
    "PCA_COMPONENTS = 200\n",
    "SILHOUETTE_SAMPLE_SIZE = 40_000 if MODE_BIG else None\n",
    "\n",
    "# Sampling constants\n",
    "EVAL_ROWS_PER_USER = 1\n",
    "GROUP_USER_INTERACTIONS_BY_TIMESTAMP = True\n",
    "MAX_PROFILE_SIZE = None\n",
    "TOTAL_SAMPLES_TRAIN = 10_000_000\n",
    "TOTAL_SAMPLES_VALID = 500_000\n",
    "\n",
    "# Sampling constants (original)\n",
    "ARTIST_BOOST = 0.2\n",
    "CONFIDENCE_MARGIN = 0.18\n",
    "FAKE_COEF = 0. if MODE_PROFILE is True else 0.\n",
    "FINE_GRAINED_THRESHOLD = 0.7\n",
    "assert all(\n",
    "    0. <= var <= 1.\n",
    "    for var in [ARTIST_BOOST, CONFIDENCE_MARGIN, FINE_GRAINED_THRESHOLD, FAKE_COEF]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed... ({RNG_SEED})\")\n",
    "    random.seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
    "embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "\n",
    "# Extract features and \"id2index\" mapping\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features, item_id2index, _ = extract_embedding(embedding, verbose=True)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "del embedding  # Release some memory\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Additional embeddings\n",
    "\n",
    "# Color embedding\n",
    "COLOR_EMBEDDING_PATH = os.path.join(\"..\", \"..\", \"temp2\", \"colors_conv1.npy\")\n",
    "color_embedding = np.load(COLOR_EMBEDDING_PATH, allow_pickle=True)\n",
    "color_embedding = color_embedding.astype(float)\n",
    "\n",
    "# Texture embedding\n",
    "TEXTURE_EMBEDDING_PATH = os.path.join(\"..\", \"..\", \"temp2\", \"texture_relu.npy\")\n",
    "texture_embedding = np.load(TEXTURE_EMBEDDING_PATH, allow_pickle=True)\n",
    "texture_embedding = texture_embedding.astype(float)\n",
    "\n",
    "# Concatenation\n",
    "features = np.concatenate((color_embedding, texture_embedding), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating visual clusters\n",
    "if USE_CACHE:\n",
    "    print(f\"\\nCreating visual clusters (steps 1 & 2): Loading cached PCA ({CACHE_PCA_PATH})\")\n",
    "    features = np.load(CACHE_PCA_PATH, allow_pickle=True)\n",
    "else:\n",
    "    print(\"\\nCreating visual clusters (step 1): z-score normalization of embedding\")\n",
    "    if MODE_BIG:\n",
    "        features = StandardScaler().partial_fit(features).transform(features)\n",
    "    else:\n",
    "        features = StandardScaler().fit_transform(features)\n",
    "    print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "    print(\"\\nCreating visual clusters (step 2): Conduct PCA to reduce dimension\")\n",
    "    if MODE_BIG:\n",
    "        features = IncrementalPCA(n_components=PCA_COMPONENTS).fit_transform(features)\n",
    "    else:\n",
    "        features = PCA(n_components=PCA_COMPONENTS).fit_transform(features)\n",
    "    np.save(CACHE_PCA_PATH, features, allow_pickle=True)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "\n",
    "if USE_CACHE:\n",
    "    print(f\"\\nCreating visual clusters (step 3): Loading cached labels ({CACHE_LABELS_PATH})\")\n",
    "    clusterer_labels = np.load(CACHE_LABELS_PATH, allow_pickle=True)\n",
    "    best_score = silhouette_score(features, clusterer_labels, sample_size=SILHOUETTE_SAMPLE_SIZE)\n",
    "else:\n",
    "    print(\"\\nCreating visual clusters (step 3): Perform k-means clustering\")\n",
    "    best_score = float(\"-inf\")\n",
    "    best_clusterer = None\n",
    "    for i in range(CLUSTERING_N_TIMES):\n",
    "        if MODE_BIG:\n",
    "            clusterer = MiniBatchKMeans(\n",
    "                n_clusters=CLUSTERING_N_CLUSTERS,\n",
    "                batch_size=2000,\n",
    "                max_iter=2000,\n",
    "                n_init=CLUSTERING_N_INIT,\n",
    "                random_state=CLUSTERING_RNG,\n",
    "            ).fit(features)\n",
    "        else:\n",
    "            clusterer = KMeans(\n",
    "                n_clusters=CLUSTERING_N_CLUSTERS,\n",
    "                max_iter=2000,\n",
    "                n_init=CLUSTERING_N_INIT,\n",
    "                random_state=CLUSTERING_RNG,\n",
    "            ).fit(features)\n",
    "        score = silhouette_score(features, clusterer.labels_, sample_size=SILHOUETTE_SAMPLE_SIZE)\n",
    "        if score > best_score:\n",
    "            best_clusterer = clusterer\n",
    "            best_score = score\n",
    "        if CLUSTERING_RNG is not None:\n",
    "            break\n",
    "        print(f\">> Silhouette score ({i + 1:02}/{CLUSTERING_N_TIMES:02}): {score:.6f}\")\n",
    "    clusterer_labels = best_clusterer.labels_\n",
    "    del best_clusterer  # Release some memory\n",
    "    np.save(CACHE_LABELS_PATH, clusterer_labels, allow_pickle=True)\n",
    "print(f\">> Best Silhouette score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interactions CSVs\n",
    "print(f\"\\nLoading interactions from files...\")\n",
    "interactions_df = get_interactions_dataframe(\n",
    "    INTERACTIONS_PATH,\n",
    "    display_stats=True,\n",
    ")\n",
    "\n",
    "# Apply 'item_id2index', to work with indexes only\n",
    "print(\"\\nApply 'item_id2index' mapping for items...\")\n",
    "interactions_df[\"item_id\"] = interactions_df[\"item_id\"].map(str)\n",
    "n_missing_ids = interactions_df[~interactions_df[\"item_id\"].isin(item_id2index)][\"item_id\"].count()\n",
    "interactions_df = interactions_df[interactions_df[\"item_id\"].isin(item_id2index)]\n",
    "interactions_df[\"item_id\"] = interactions_df[\"item_id\"].map(item_id2index)\n",
    "print(f\">> Mapping applied, ({n_missing_ids} values in 'item_id2index')\")\n",
    "\n",
    "# Store mapping from user_id to index (0-index, no skipping)\n",
    "print(\"\\nCreate 'user_id2index' mapping for users...\")\n",
    "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
    "new_user_ids = np.argsort(unique_user_ids)\n",
    "user_id2index = dict(zip(unique_user_ids, new_user_ids))\n",
    "\n",
    "# Apply 'user_id2index', to work with indexes only\n",
    "print(\"\\nApply 'user_id2index' mapping for users...\")\n",
    "n_missing_ids = interactions_df[~interactions_df[\"user_id\"].isin(user_id2index)][\"user_id\"].count()\n",
    "interactions_df = interactions_df[interactions_df[\"user_id\"].isin(user_id2index)]\n",
    "interactions_df[\"user_id\"] = interactions_df[\"user_id\"].map(user_id2index)\n",
    "print(f\">> Mapping applied, ({n_missing_ids} values in 'user_id2index')\")\n",
    "\n",
    "# Create helper mapping from idx to data\n",
    "print(\"\\nCreating mappings from index to data\")\n",
    "artist_by_idx = np.full((features.shape[0],), -1)\n",
    "for item_id, artist_id in interactions_df.set_index(\"item_id\").to_dict()[\"artist_id\"].items():\n",
    "    artist_by_idx[item_id] = artist_id\n",
    "cluster_by_idx = clusterer_labels\n",
    "\n",
    "# Create helper mapping from data to idxs\n",
    "print(\"\\nCreating mappings from data to index\")\n",
    "artistId2artworkIndexes = interactions_df.groupby(\"artist_id\")[\"item_id\"].apply(list).to_dict()\n",
    "clustId2artIndexes = dict()\n",
    "for i, cluster in enumerate(cluster_by_idx):\n",
    "    if cluster not in clustId2artIndexes:\n",
    "        clustId2artIndexes[cluster] = list()\n",
    "    clustId2artIndexes[cluster].append(i)\n",
    "\n",
    "# Form interactions baskets, grouping by timestamp and user_id\n",
    "if GROUP_USER_INTERACTIONS_BY_TIMESTAMP:\n",
    "    print(\"\\nForm interactions groups (baskets), by timestamp and user_id...\")\n",
    "    interactions_df = interactions_df.groupby([\"timestamp\", \"user_id\"])[\"item_id\"].apply(list)\n",
    "    interactions_df = interactions_df.reset_index()\n",
    "    interactions_df = interactions_df.sort_values(\"timestamp\")\n",
    "    interactions_df = interactions_df.reset_index(drop=True)\n",
    "else:\n",
    "    print(\"\\nInteractions groups (baskets), by timestamp and user_id, skipped\")\n",
    "    \n",
    "# Mark interactions used for evaluation procedure\n",
    "print(\"\\nApply evaluation split...\")\n",
    "interactions_df = mark_evaluation_rows(interactions_df, threshold=EVAL_ROWS_PER_USER)\n",
    "# Check if new column exists and has boolean dtype\n",
    "assert interactions_df[\"evaluation\"].dtype.name == \"bool\"\n",
    "print(f\">> Interactions: {interactions_df.shape}\")\n",
    "\n",
    "# Split interactions data according to evaluation column\n",
    "evaluation_df, interactions_df = get_holdout(interactions_df)\n",
    "assert not interactions_df.empty\n",
    "assert not evaluation_df.empty\n",
    "print(f\">> Evaluation: {evaluation_df.shape} | Interactions: {interactions_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating helpers instances...\")\n",
    "# Creating hashes container for duplicates detection\n",
    "hashes_container = HashesContainer()\n",
    "# Creating custom score helpers\n",
    "vissimhandler = VisualSimilarityHandler(clusterer_labels, features)\n",
    "hybrid_scorer = HybridScorer(vissimhandler, artist_by_idx, artist_boost=ARTIST_BOOST)\n",
    "\n",
    "# Sampling constants\n",
    "print(\"\\nCalculating important values...\")\n",
    "N_REAL_STRATEGIES = 2\n",
    "N_FAKE_STRATEGIES = 2\n",
    "print(f\">> There are {N_REAL_STRATEGIES} real strategies and {N_FAKE_STRATEGIES} fake strategies\")\n",
    "N_SAMPLES_PER_REAL_STRAT_TRAIN = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_TRAIN / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_REAL_STRAT_VALID = ceil((1 - FAKE_COEF) * TOTAL_SAMPLES_VALID / N_REAL_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_TRAIN = ceil(FAKE_COEF * TOTAL_SAMPLES_TRAIN / N_FAKE_STRATEGIES)\n",
    "N_SAMPLES_PER_FAKE_STRAT_VALID = ceil(FAKE_COEF * TOTAL_SAMPLES_VALID / N_FAKE_STRATEGIES)\n",
    "N_USERS = interactions_df[\"user_id\"].nunique()\n",
    "N_ITEMS = len(features)\n",
    "print(f\">> N_USERS = {N_USERS} | N_ITEMS = {N_ITEMS}\")\n",
    "\n",
    "# Actual sampling section\n",
    "print(\"\\nCreating samples using custom strategies...\")\n",
    "strategy_handler = StrategyHandler(\n",
    "    interactions_df,\n",
    "    vissimhandler, hybrid_scorer,\n",
    "    clustId2artIndexes, cluster_by_idx,\n",
    "    artistId2artworkIndexes, artist_by_idx,\n",
    "    threshold=FINE_GRAINED_THRESHOLD,\n",
    "    confidence_margin=CONFIDENCE_MARGIN,\n",
    "    user_as_items=MODE_PROFILE,\n",
    "    max_profile_size=MAX_PROFILE_SIZE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Strategy #1: Given real profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_1 = strategy_handler.strategy_1(\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_1) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_1 = strategy_handler.strategy_1(\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_1) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #1 Training samples ({len(samples_train_1)}) and validation samples ({len(samples_valid_1)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Strategy #2: Given fake profile, recommend profile\")\n",
    "# Sampling training samples\n",
    "samples_train_2 = strategy_handler.strategy_2(\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_2) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_2 = strategy_handler.strategy_2(\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_2) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #2: Training samples ({len(samples_train_2)}) and validation samples ({len(samples_valid_2)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Strategy #3: Given real profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_3 = strategy_handler.strategy_3(\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_TRAIN / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_3) >= N_SAMPLES_PER_REAL_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_3 = strategy_handler.strategy_3(\n",
    "    ceil(N_SAMPLES_PER_REAL_STRAT_VALID / N_USERS),  # samples_per_user\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_3) >= N_SAMPLES_PER_REAL_STRAT_VALID\n",
    "print(f\">> Strategy #3: Training samples ({len(samples_train_3)}) and validation samples ({len(samples_valid_3)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">> Strategy #4: Given fake profile, recommend items according to hybrid scorer\")\n",
    "# Sampling training samples\n",
    "samples_train_4 = strategy_handler.strategy_4(\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_TRAIN / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_train_4) >= N_SAMPLES_PER_FAKE_STRAT_TRAIN\n",
    "# Sampling validation samples\n",
    "samples_valid_4 = strategy_handler.strategy_4(\n",
    "    ceil(N_SAMPLES_PER_FAKE_STRAT_VALID / N_ITEMS),  # samples_per_item\n",
    "    hashes_container,  # hashes_container\n",
    ")\n",
    "assert len(samples_valid_4) >= N_SAMPLES_PER_FAKE_STRAT_VALID\n",
    "print(f\">> Strategy #4: Training samples ({len(samples_train_4)}) and validation samples ({len(samples_valid_4)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log out detected collisions\n",
    "print(f\"\\nLog detected collisions...\")\n",
    "print(f\">> Total hash collisions: {hashes_container.collisions}\")\n",
    "print(f\">> Total visual collisions: {vissimhandler.count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge triples into a single list\n",
    "print(\"\\nMerging strategies samples into a single list\")\n",
    "TRAINING_DATA = [samples_train_1, samples_train_2, samples_train_3, samples_train_4]\n",
    "for i, samples in enumerate(TRAINING_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):8d} | Sample: {samples[0] if samples else None}\")\n",
    "TRAINING_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in TRAINING_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Training samples: {len(TRAINING_DATA)}\")\n",
    "# Merge strategies samples\n",
    "VALIDATION_DATA = [samples_valid_1, samples_valid_2, samples_valid_3, samples_valid_4]\n",
    "for i, samples in enumerate(VALIDATION_DATA, start=1):\n",
    "    print(f\">> Strategy {i}: Size: {len(samples):8d} | Sample: {samples[0] if samples else None}\")\n",
    "VALIDATION_DATA = [\n",
    "    triple\n",
    "    for strategy_samples in VALIDATION_DATA\n",
    "    for triple in strategy_samples\n",
    "]\n",
    "print(f\">> Validation samples: {len(VALIDATION_DATA)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for duplicated hashes\n",
    "print(f\"\\nNaive triples validation and looking for duplicates...\")\n",
    "validation_hash_check = HashesContainer()\n",
    "all_samples = [\n",
    "    triple\n",
    "    for subset in (TRAINING_DATA, VALIDATION_DATA)\n",
    "    for triple in subset\n",
    "]\n",
    "user_ids = interactions_df[\"user_id\"].unique()\n",
    "user_data = dict()\n",
    "for triple in tqdm(all_samples, desc=\"Naive validation\"):\n",
    "    profile, pi, ni, ui = triple\n",
    "    if MODE_PROFILE:\n",
    "        assert validation_hash_check.enroll(pre_hash((profile, pi, ni)))\n",
    "    else:\n",
    "        assert validation_hash_check.enroll(pre_hash((ui, pi, ni), contains_iter=False))\n",
    "    assert 0 <= pi < N_ITEMS\n",
    "    assert 0 <= ni < N_ITEMS\n",
    "    assert pi != ni\n",
    "    assert not vissimhandler.same(pi, ni)\n",
    "    if ui == -1:\n",
    "        continue\n",
    "    assert ui in user_ids\n",
    "    if not ui in user_data:\n",
    "        user = interactions_df[interactions_df[\"user_id\"] == ui]\n",
    "        user_data[ui] = set(np.hstack(user[\"item_id\"].values))\n",
    "    user_artworks = user_data[ui]\n",
    "    assert all(i in user_artworks for i in profile)\n",
    "    spi = hybrid_scorer.get_score(ui, user_artworks, pi)\n",
    "    sni = hybrid_scorer.get_score(ui, user_artworks, ni)\n",
    "    assert spi > sni\n",
    "print(\">> No duped hashes found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating output files (train and valid)...\")\n",
    "# Training dataframe\n",
    "df_train = pd.DataFrame(TRAINING_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_train[\"profile\"] = df_train[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving training samples ({OUTPUT_TRAIN_PATH})\")\n",
    "df_train.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "\n",
    "# Validation dataframe\n",
    "df_validation = pd.DataFrame(VALIDATION_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_validation[\"profile\"] = df_validation[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving validation samples ({OUTPUT_VALID_PATH})\")\n",
    "df_validation.to_csv(OUTPUT_VALID_PATH, index=False)\n",
    "\n",
    "# Evaluation dataframe\n",
    "df_evaluation = evaluation_df.copy()\n",
    "if GROUP_USER_INTERACTIONS_BY_TIMESTAMP:\n",
    "    df_evaluation[\"predict\"] = df_evaluation[\"predict\"].map(lambda l: \" \".join(map(str, l)))\n",
    "df_evaluation[\"profile\"] = df_evaluation[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving evaluation data ({OUTPUT_EVAL_PATH})\")\n",
    "df_evaluation.to_csv(OUTPUT_EVAL_PATH, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.8.5",
   "language": "python",
   "name": "3.8.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
